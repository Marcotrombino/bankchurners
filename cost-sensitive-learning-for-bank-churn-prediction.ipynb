{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n1. [Context and goals](#Context-and-goals)\n2. [Dataset description](#Dataset-description)\n2. [Data cleaning](#Data-cleaning)\n3. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n    1.   [Sociodemographic variables](#Sociodemographic-variables)\n    2.   [Service usage variables](#Service-usage-variables)\n    3.   [Conditioned Analysis](#Conditional-Analysis)\n    4.   [EDA summary](#EDA-summary)\n4. [Predictive Analysis](#Predictive-Analysis)\n    1. [Evaluation metrics](#Evaluation-metrics)\n    2. [Decision Trees](#Decision-Trees)\n    3. [Gradient Boosting](#Gradient-Boosting)\n    4. [Comparison and Conclusion](#Comparison)"},{"metadata":{},"cell_type":"markdown","source":"# Context and goals\nThe case study is about a bank manager worried about the increasing number of churns in the credit card service. In a business, the cost to get a new customer is usually much **higher** than what it takes to keep an existing one. For this purpose, the main goal will be **predicting** the highest number of **potential churners** to let the manager proactively propose better offers to customers.\n\nThis notebook consists of a preliminary exploratory data analysis and a predictive analysis by using **decision trees** and ensemble methods (**Gradient Boosting**)."},{"metadata":{},"cell_type":"markdown","source":"# Dataset description\nThe dataset consists of 10127 observations, one for each different customer bank's account. For each account the provided relevant information are the following:\n* **Attrition_Flag** : Wether the user leaved or not the service (bank's account closed or not)\n* **Customer_Age** : Customer's age in year\n* **Gender** : Customer's gender (male or female)\n* **Dependent_count** : Number of people who depend upon the customer for their support and welfare.\n* **Education_Level** : Customer's educational qualification (high school, graduate, etc.)\n* **Marital_Status** : Customer's marital status (married, single, etc.)\n* **Income_Category** :Customer's income bracket in dollars (less than 40K, 40K-60K, etc.)\n* **Card_Category** : Credit card category (Blue, Silver, etc.)\n* **Months_on_book** : Period of relationship with bank in months\n* **Total_Relationship_Count** : Total number of products held by the customer\n* **Months_Inactive_12_mon** : Number of months inactive in the last 12 months\n* **Contacts_Count_12_mon** : Number of contacts (phone calls) in the last 12 months\n* **Credit_Limit** : Credit limit on the credit card\n* **Total_Revolving_Bal** : Total revolving balance on the credit card\n* **Avg_Open_To_Buy** : Open to buy credit line (average of last 12 months). This also turns out to be the difference between the credit limit (Credit_Limit) assigned to a cardholder account and the present balance on the account (Total_Revolving_Bal).\n* **Total_Amt_Chng_Q4_Q1** : Change in total transactions amount (Q4 over Q1) \n* **Total_Trans_Amt** : Total transactions amount (last 12 months)\n* **Total_Trans_Ct** : Total number of transactions (last 12 months)\n* **Total_Ct_Chng_Q4_Q1** : Change in total number of transactions (Q4 over Q1)\n* **Avg_Utilization_Ratio** : Average card utilization ratio"},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Import libraries\nlibrary(tidyverse)\nlibrary(forcats)\nlibrary(psych)\nlibrary(gridExtra)\nlibrary(rlang) \ninstall.packages(\"gghalves\")\nlibrary(gghalves)\nlibrary(ggrepel)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# load data\nbank_data_raw <- read_csv(\"../input/credit-card-customers/BankChurners.csv\")\n\n# --- Remove useless information (account id and last two columns) ---\nbank_data_raw <- bank_data_raw %>% \nselect(-c(\n  CLIENTNUM,\n  Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1,\n  Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\n))\n\nbank_data_raw_2 <- bank_data_raw\n\n# --- Attrition_Flag encoding: 0 Existing Customer, 1 Attrited Customer ---\nbank_data_raw_2 <- bank_data_raw_2 %>% mutate(Attrition_Flag = recode(Attrition_Flag, \"Attrited Customer\" = 1, \"Existing Customer\" = 0))\n\nbank_data <- bank_data_raw_2\n\n# --- Factor conversion and levels reordering ---\n\nbank_data$Attrition_Flag <- as_factor(bank_data$Attrition_Flag)\n\nbank_data$Gender <- as_factor(bank_data$Gender)\n\nbank_data$Education_Level <- as_factor(bank_data$Education_Level)\nbank_data$Education_Level <- fct_relevel(bank_data$Education_Level, \"Unknown\", \"Uneducated\", \"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\")\n\nbank_data$Marital_Status <- as_factor(bank_data$Marital_Status)\nbank_data$Marital_Status <- fct_relevel(bank_data$Marital_Status, \"Unknown\", \"Single\", \"Married\", \"Divorced\")\n\nbank_data$Income_Category <- as_factor(bank_data$Income_Category)\nbank_data$Income_Category <- fct_relevel(bank_data$Income_Category, \"Unknown\", \"Less than $40K\", \"$40K - $60K\", \"$60K - $80K\", \"$80K - $120K\", \"$120K +\")\n\nbank_data$Card_Category <- as_factor(bank_data$Card_Category)\nbank_data$Card_Category <- fct_relevel(bank_data$Card_Category, \"Blue\", \"Silver\", \"Gold\", \"Platinum\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing data\n\nThe dataset contains few **missing data** with values \"Unknown\" in *Education_Level*, *Marital_Status*, *Income_Category*. \n\nThese data have been considered **not imputable** because they may not heavily depend on other variables. Also, due to the low frequency of this modality, imputation would not provide any drastic improvement in the results."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nSociodemographic and service usage variables have been analyzed separately. Also, a conditioned analysis has been conducted to better understand which variables could be more relevant to separate churners from not churners."},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width = 14, repr.plot.height = 8)\n\ngender_fill_scale <- scale_fill_manual(\n  values = c(\"#4292c6\", \"#fb6a4a\"), \n  breaks = c(\"M\", \"F\"), \n  labels = c(\"M\", \"F\")\n) \n\ncard_fill_scale <- scale_fill_manual(\n  values = c(\"#08519c\", \"#bbbbbb\", \"#d3a826\", \"#f6f9fb\"), \n  breaks = c(\"Blue\", \"Silver\", \"Gold\", \"Platinum\"), \n  labels = c(\"Blue\", \"Silver\", \"Gold\", \"Platinum\")\n) \n\nattrition_fill_scale <- scale_fill_manual(\n  values = c(\"#7FC97F\", \"#FDC086\"), \n  breaks = c(0, 1), \n  labels = c(\"0\", \"1\")\n)\n\nattrition_color_scale <- scale_color_manual(\n  values = c(\"#7FC97F\", \"#FDC086\"), \n  breaks = c(0, 1), \n  labels = c(\"0\", \"1\")\n)\n\n# --- Histogram utility function ---\nget_histogram <- function(data, var, binwidth = 30) {\n  p <- data %>%\n    ggplot() +\n    geom_histogram(aes(x = get(var)), color = \"black\", fill = \"#f6f9fb\", binwidth = binwidth) +\n    geom_vline(aes(xintercept = summary(get(var))[4], linetype=\"mean\"), size=1.3) +\n    geom_vline(aes(xintercept = summary(get(var))[3], linetype=\"median\"), size=1.3) +\n    theme_classic() +\n    labs(\n      x = var,\n      y = \"frequency\",\n      title = paste(var, \"distribution\"),\n      linetype = \"measure\"\n    )\n  \n  return(p)\n}\n\n# --- Box plot utility function\nget_boxplot <- function(data, var) {\n  data %>%\n    ggplot() +\n    geom_boxplot(aes(x = get(var)), alpha = 0.7, color = \"black\", fill = \"#f6f9fb\") +\n    theme_classic() +\n    labs(\n      x = var,\n      title = paste(var, \"distribution\")\n    )\n}\n\n# --- Barplot with absolute and percentage frequencies utility function ---\nget_var_freq_mixed <- function(data, var, palette) {\n    \n  p <- data %>%\n    count(get(var)) %>%\n    mutate(pct = n / sum(n),\n           pctlabel = paste0(n, \" (\", round(pct*100, 2), \"%)\")) %>%\n    ggplot(aes(x = factor(`get(var)`), y = n, fill = factor(`get(var)`))) + \n    geom_bar(stat = \"identity\", color = \"black\") +\n    geom_text(aes(label = pctlabel), vjust = -0.25, size = 4) +\n    theme_classic() +\n    labs(\n      x = var,\n      y = \"Absolute frequency\",\n      fill = var,\n      title = paste(var, \"distribution\")\n    )\n  \n  if(hasArg(palette)) {\n    p <- p + scale_fill_brewer(palette=palette)\n  }\n  \n  return(p)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sociodemographic variables\n\nSix variables provide sociodemographic information: \n* *Customer_Age*\n* *Gender*\n* *Dependent_count*\n* *Education_Level*\n* *Marital_Status*\n* *Income_Category*\n"},{"metadata":{},"cell_type":"markdown","source":"### Customer_Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Customer_Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sd(bank_data$Customer_Age) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Customer_Age\", binwidth = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Gender\") + gender_fill_scale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dependent_count"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Dependent_count\", palette = \"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Education_Level\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Education_Level\", palette = \"Reds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Marital_Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Marital_Status\", palette = \"Spectral\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Income_Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Income_Category\", palette = \"Greens\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Service usage variables\nThirteen variables provide service usage information:\n\n* *Card_Category*\n* *Months_on_book*\n* *Total_Relationship_Count*\n* *Months_Inactive_12_mon*\n* *Contacts_Count_12_mon*\n* *Credit_Limit*\n* *Total_Revolving_Bal*\n* *Avg_Open_To_Buy*\n* *Total_Amt_Chng_Q4_Q1*\n* *Total_Trans_Amt*\n* *Total_Trans_Ct*\n* *Total_Ct_Chng_Q4_Q1*\n* *Avg_Utilization_Ratio*"},{"metadata":{},"cell_type":"markdown","source":"### Card_Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Card_Category\") + card_fill_scale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Months_on_book"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Months_on_book)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sd(bank_data$Months_on_book) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Months_on_book\", binwidth = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Relationship_count"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Total_Relationship_Count\", palette = \"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Months_Inactive_12_mon"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Months_Inactive_12_mon\", palette = \"Reds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Contacts_Count_12_mon"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_var_freq_mixed(bank_data, var = \"Contacts_Count_12_mon\", palette = \"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Credit_Limit"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Credit_Limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR(bank_data$Credit_Limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Credit_Limit\", binwidth = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_boxplot(bank_data, var = \"Credit_Limit\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Revolving_Bal"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Total_Revolving_Bal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Total_Revolving_Bal\", binwidth = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1st mode\nbank_data %>%\n    filter(Total_Revolving_Bal %in% c(0:50)) %>%\n    count(Total_Revolving_Bal) %>%\n    arrange(desc(n)) %>%\n    head(1)\n\n# 2nd mode\nbank_data %>%\n    filter(Total_Revolving_Bal %in% c(2000:2517)) %>%\n    count(Total_Revolving_Bal) %>%\n    arrange(desc(n)) %>%\n    head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Avg_Open_To_Buy"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Avg_Open_To_Buy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR(bank_data$Avg_Open_To_Buy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Avg_Open_To_Buy\", binwidth = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_boxplot(bank_data, var = \"Avg_Open_To_Buy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Amt_Chng_Q4_Q1"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Total_Amt_Chng_Q4_Q1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR(bank_data$Total_Amt_Chng_Q4_Q1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Total_Amt_Chng_Q4_Q1\", binwidth = 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_boxplot(bank_data, var = \"Total_Amt_Chng_Q4_Q1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Trans_Amt"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Total_Trans_Amt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR(bank_data$Total_Trans_Amt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Total_Trans_Amt\", binwidth = 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_boxplot(bank_data, var = \"Total_Trans_Amt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Trans_Ct"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Total_Trans_Ct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.arrange(\n    get_histogram(bank_data, var = \"Total_Trans_Ct\", binwidth = 5),\n    get_histogram(bank_data %>% filter(Total_Trans_Ct %in% c(30:90)), var = \"Total_Trans_Ct\", binwidth = 1)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1st mode\nbank_data %>%\n    filter(Total_Trans_Ct %in% c(0:50)) %>%\n    count(Total_Trans_Ct) %>%\n    arrange(desc(n)) %>%\n    head(1)\n\n# 2nd mode\nbank_data %>%\n    filter(Total_Trans_Ct %in% c(50:100)) %>%\n    count(Total_Trans_Ct) %>%\n    arrange(desc(n)) %>%\n    head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total_Ct_Chng_Q4_Q1"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Total_Ct_Chng_Q4_Q1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR(bank_data$Total_Ct_Chng_Q4_Q1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Total_Ct_Chng_Q4_Q1\", binwidth = 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_boxplot(bank_data, var = \"Total_Ct_Chng_Q4_Q1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Avg_Utilization_Ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(bank_data$Avg_Utilization_Ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_histogram(bank_data, var = \"Avg_Utilization_Ratio\", binwidth = 0.03)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conditioned Analysis\n\nTo better understand which variables could be more relevant to separate churners from not churners, every variable has been conditioned to the response variable *Attrition_Flag* (1 = churn, 0 = not churn)."},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width = 20, repr.plot.height = 10)\n\n# Percentage bar plot conditioned to Attrition_Flag helper function\nget_var_freq_by_attrition <- function(data, var, palette) {\n    p <- data %>%\n      count(Attrition_Flag, get(var)) %>%\n      group_by(Attrition_Flag) %>%\n      mutate(pct = n / sum(n),\n             pctlabel = paste0(round(pct*100, 2), \"%\")) %>%\n      ggplot(aes(x = factor(`get(var)`), y = pct, fill = factor(`get(var)`))) + \n      geom_bar(stat = \"identity\", color = \"black\") +\n      geom_text(aes(label = pctlabel), vjust = -0.20) +\n      theme_classic() +\n      facet_wrap(~ Attrition_Flag, nrow = 2) +\n      labs(\n        x = var,\n        y = \"Percentage frequency\",\n        fill = var,\n        title = paste(\"Percentage frequencies of\", var, \"by Attrition_Flag\")\n      ) \n\n  if(hasArg(palette)) {\n    p <- p + scale_fill_brewer(palette=palette)\n  }\n  \n  return(p)\n}\n\n# Box plot conditioned to Attrition_Flag helper function\nget_boxplot_by_attrition <- function(data, var) {\n  data %>%\n    ggplot() +\n    geom_boxplot(aes(y = Attrition_Flag, x = get(var), fill = Attrition_Flag), outlier.size = 0.5) +\n    attrition_fill_scale +\n    theme_classic() +\n    labs(\n      x = var,\n      title = paste(var, \"by Attrition_Flag\")\n    )\n}\n\nget_boxviolinplot_by_attrition <- function(data, var) {\n  p <- data %>%\n    ggplot() +\n    geom_half_violin(aes(x = Attrition_Flag, y = get(var), fill = Attrition_Flag), alpha = 0.3, colour = \"grey\", side = \"r\", width = 1.3) +\n    geom_half_boxplot(aes(x = Attrition_Flag, y = get(var)), width = 0.3, outlier.size = 0.) +\n    attrition_fill_scale +\n    attrition_color_scale +\n    theme_classic() +\n    coord_flip() +\n    labs(\n      y = var,\n      title = paste(var, \"by Attrition_Flag\")\n    )\n  \n  return(p)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discrete variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.arrange(\n    ncol = 3, \n    get_var_freq_by_attrition(bank_data, var = \"Gender\") + gender_fill_scale,\n    get_var_freq_by_attrition(bank_data, var = \"Dependent_count\", palette = \"Blues\"),\n    get_var_freq_by_attrition(bank_data, var = \"Education_Level\", palette = \"Reds\")\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.arrange(\n    ncol = 3, \n    get_var_freq_by_attrition(bank_data, var = \"Marital_Status\", palette = \"Spectral\"),\n    get_var_freq_by_attrition(bank_data, var = \"Income_Category\", palette = \"Greens\"),\n    get_var_freq_by_attrition(bank_data, var = \"Card_Category\") + card_fill_scale\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.arrange(\n    ncol = 3, \n    get_var_freq_by_attrition(bank_data, var = \"Total_Relationship_Count\", palette = \"Blues\"),\n    get_var_freq_by_attrition(bank_data, var = \"Months_Inactive_12_mon\", palette = \"Reds\"),\n    get_var_freq_by_attrition(bank_data, var = \"Contacts_Count_12_mon\", palette = \"Blues\")\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Continuous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.arrange(\n    nrow = 2,\n    get_boxplot_by_attrition(bank_data, \"Customer_Age\"),\n    get_boxplot_by_attrition(bank_data, \"Months_on_book\"),\n    get_boxplot_by_attrition(bank_data, \"Credit_Limit\"),\n    get_boxplot_by_attrition(bank_data, \"Total_Revolving_Bal\"),\n    get_boxplot_by_attrition(bank_data, \"Avg_Open_To_Buy\"),\n    get_boxplot_by_attrition(bank_data, \"Total_Amt_Chng_Q4_Q1\"),\n    get_boxplot_by_attrition(bank_data, \"Total_Trans_Amt\"),\n    get_boxplot_by_attrition(bank_data, \"Total_Trans_Ct\"),\n    get_boxplot_by_attrition(bank_data, \"Total_Ct_Chng_Q4_Q1\"),\n    get_boxplot_by_attrition(bank_data, \"Avg_Utilization_Ratio\")\n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"suppressWarnings(\n    grid.arrange(\n        nrow = 2,\n        get_boxviolinplot_by_attrition(bank_data, \"Customer_Age\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Months_on_book\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Credit_Limit\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Revolving_Bal\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Avg_Open_To_Buy\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Amt_Chng_Q4_Q1\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Trans_Amt\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Trans_Ct\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Ct_Chng_Q4_Q1\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Avg_Utilization_Ratio\")\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# continuous variables subset\ngrid.arrange(\n    nrow = 2,\n    get_boxplot_by_attrition(bank_data, \"Total_Trans_Ct\"),\n    get_boxplot_by_attrition(bank_data, \"Total_Trans_Amt\"),\n    get_boxplot_by_attrition(bank_data, \"Total_Ct_Chng_Q4_Q1\"),\n    get_boxplot_by_attrition(bank_data, \"Total_Revolving_Bal\")\n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# continuous variables subset\nsuppressWarnings(\n    grid.arrange(\n        nrow = 2,\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Trans_Ct\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Trans_Amt\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Ct_Chng_Q4_Q1\"),\n        get_boxviolinplot_by_attrition(bank_data, \"Total_Revolving_Bal\")\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA summary\n\n**Sociodemographic** variables:\n* Customers' age ranges from 26 to 73 years (mean 46.33 Â± 8.017)\n* 47.09% of customers are males, whereas 52.91% are females.\n* More than 91% of customers have at least one person who depends upon them for support and welfare, whereas more than 50% have two or three people.\n* Most frequent qualifications are \"Graduate\" (30.89%) and \"High School\" (19.88%). 15% of observations have no information.\n* Most frequent marital status is \"Married\" (46.28%), followed by \"Single\" (38.94%). Only 7.4% of customers are divorced.\n* More than 50% of customers have an income higher than 40K dollars. The most frequent income bracket is 40K dollars.\n\nMost relevant **service usage** variables:\n* The period of relationship with bank ranges from 13 to 56 months (mean 35.93 Â± 7.99)\n* More than 92% of customers have been inactive between 1 and 3 months in the last 12 months. Only 29 customers (0.29%) haven't been inactive in the last 12 months\n* More than 78% of customers hold at least 3 or more products. Only the 8.99% hold just one product.\n* The total number of transactions made in the last 12 months ranges from 10 to 139 transactions, with two modes at 43 and 81 (highest one)\n* The total transactions amount made in the last 12 months ranges from 510 to 18484 dollars, with a median equal to 3900 dollars and an interquartile range of 2156-4741   \n* The total revolving balance on the credit card ranges from 0 to 2517 dollars (these two values are also the modes). \n\nVariables with an observable **separation** for the response variable:\n* *Total_Relationship_Count*, *Months_Inactive_12_mon* and *Contacts_Count_12_mon* shows some percentage differences when conditioned to Attrition_Flag\n* *Total_Trans_Ct*, *Total_Trans_Amt*, *Total_Ct_Chng_Q4_Q1* and *Total_Revolving_Bal* distributions show a separation when conditioned to Attrition_Flag"},{"metadata":{},"cell_type":"markdown","source":"# Predictive Analysis\n\nThe problem has been analyzed as a **binary classification** considering that the response (Attrition_Flag) is a dichotomous variable. In this respect, a 75/25% train-test split has been used and [other 19 variables](#Dataset-description) have been considered as explanatory variables. K-Fold cross-validation has been used to evaluate models during the training phase."},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(123)\nsel <- sample(1: nrow(bank_data), size = (nrow(bank_data)*75)/100, replace = FALSE)\n\n# train - test split (75% - 25%)\ntrain <- bank_data[sel, ]\ntest <- bank_data[-sel, ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table(bank_data$Attrition_Flag)\ntable(train$Attrition_Flag)\ntable(test$Attrition_Flag)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation metrics\nWhen facing a classification problem, **confusion matrix** is generally used to obtain evaluation metrics such as [accuracy](https://en.wikipedia.org/wiki/Precision_and_recall). \n\nHowever, it's important to underline that in the present case the dataset is heavily **unbalanced** (only 16.07% of observations are about churners). \nIn such a case, accuracy can be a misleading classification metric because, for example, a simple model with only \"not churner\" as a response would have 83.93% accuracy.\n\nA more robust approach is to use the [recall](https://en.wikipedia.org/wiki/Precision_and_recall) (or sensitivity) and [precision](https://en.wikipedia.org/wiki/Precision_and_recall) (or PPV) metrics. However, these two metrics affect each other (the higher the one it is the lower the other is) by forcing to choose a **trade-off** depending on the context. In the present case, a low recall means that only a few actual churners have been identified as churners, whereas a low precision means that only a few customers predicted as churners are actually churners. Not all trade-offs are equally \"fair\" (e.g. a little increase on recall could lend to a more drastic decrease in precision), so it could be useful to have a measure of \"fairness\". \n\nTo better evaluate the two metrics, a harmonic mean called [F1-score](https://en.wikipedia.org/wiki/F-score) can be used to aggregate those two metrics and measure both performances. To achieve the main goal and still get the best of both metrics, the proposed approach is **maximizing F1**. However, with equal F1 values, **higher recall is preferable** to higher precision, as the bank may be more interested in identifying as many churners as possible in exchange for some risk to wrongly classify a non-churner as a churner and proposing them unnecessary better offers."},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width = 14, repr.plot.height = 8)\nget_var_freq_mixed(bank_data, \"Attrition_Flag\") + attrition_fill_scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification metrics helper function\nclassification_metrics <- function(confusion_matrix) { \n  metrics <- data.frame(matrix(0, nrow=1, ncol=5))\n\n  metrics[1:4]<-confusion_matrix$byClass[c(2,6,5,7)]\n  metrics[5]<-confusion_matrix$overall[1]\n  names(metrics)<-c(\"Specificity\", \"Recall\", \"Precision\", \"F1\", \"Accuracy\")\n  \n  return(metrics)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Trees"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"library(rpart)\nlibrary(rpart.plot)\nlibrary(caret)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Full Tree\nA classification tree has been generated by using a complexity parameter cp = 0.001. The resulting tree has 63 leaves."},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(123)\n\n# tree training\ntree_0 <- rpart(Attrition_Flag ~ ., data = train, method = \"class\", cp=0.001)\n\n# predict on test set\ntest_pred_tree_0<- predict(tree_0, newdata = test, type = \"class\") \n\n# get confusion matrix\nconf_matrix_tree_0 <- caret::confusionMatrix(test_pred_tree_0, test$Attrition_Flag, positive = \"1\", mode = \"everything\")\nconf_matrix_tree_0$table\n\n# get classification metrics\n(metrics_tree_0 <- classification_metrics(conf_matrix_tree_0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"F1, recall, and precision performances are already good, although precision is higher than recall. The accuracy value is pretty high as we expected from the unbalanced dataset's nature."},{"metadata":{},"cell_type":"markdown","source":"### Pruned Tree\nThe previous tree can be pruned by using the \"one-standard-error\" rule. The selected tree has 23 leaves (cp = 0.0046)"},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width = 16, repr.plot.height = 8)\nplotcp(tree_0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tree pruning\ntree_1 <- prune(tree_0, cp=0.0046)\n\n# predict on test set\ntest_pred_tree_1<- predict(tree_1, newdata = test, type = \"class\") \n\n# get confusion matrix\nconf_matrix_tree_1 <- caret::confusionMatrix(test_pred_tree_1, test$Attrition_Flag, positive = \"1\", mode = \"everything\")\nconf_matrix_tree_1$table\n\n# get classification metrics\n(metrics_tree_1 <- classification_metrics(conf_matrix_tree_1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resulting tree is simpler than the original one and it also has slightly better performance. "},{"metadata":{},"cell_type":"markdown","source":"### Prior Tree\nOne way to fix class imbalance is specifying the response variable distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(123)\n\n# tree training with explicit class frequencies\ntree_2 <- rpart(Attrition_Flag ~ ., data = train, method = \"class\", parms=list(prior=c(0.16, 0.84)))\n\n# predict on test set\ntest_pred_tree_2 <- predict(tree_2, newdata = test, type = \"class\") \n\n# get confusion matrix\nconf_matrix_tree_2 <- confusionMatrix(data = test_pred_tree_2, reference = test$Attrition_Flag, positive = \"1\")\nconf_matrix_tree_2$table\n\n# get classification metrics\n(metrics_tree_2 <- classification_metrics(conf_matrix_tree_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance shows a drastic increase in recall and likewise decreases in precision (and F1). In spite of a really high recall, the model suffers from **really low precision**, which would lead to often wrongly classify a non-churner as a churner (65% of predicted as churners actually are not churners). \n\nCost-sensitive techniques allow to better fine-tune the precision-recall trade-off."},{"metadata":{},"cell_type":"markdown","source":"### Cost-sensitivity optimization\nTo achieve a recall higher than precision, the cost of misclassification errors can be taken into account with [**Cost-Sensitive Learning**](https://machinelearningmastery.com/cost-sensitive-learning-for-imbalanced-classification/):\n> In cost-sensitive learning instead of each instance being either correctly or incorrectly classified, each class (or instance) is given a misclassification cost. Thus, instead of trying to optimize the accuracy, the problem is then to minimize the total misclassification cost.\nMost classifiers assume that the misclassification costs (false negative and false positive cost) are the same. In most real-world applications, this assumption is not true.\n\nIn the present case, a higher misclassification cost must be given to the minority class (1 = churn). This can be achieved by specifying an asymmetric cost matrix and assigning a higher cost to False Negatives. This cost can be tuned to select the best precision-recall trade-off:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# missclassification costs between 1 and 4 with step 0.1\ncosts_tree<-seq(1,4,0.1)\n\n# optimization results\noptimization_results_tree<-data.frame(matrix(0, nrow=length(costs_tree), ncol=6))\nnames(optimization_results_tree)<-c(\"Cost\", \"Specificity\", \"Recall\", \"Precision\", \"F1\", \"Accuracy\")\n\n# create a tree for each i-th cost \nfor(i in seq_along(costs_tree)) {\n  \n    set.seed(123)\n\n    # i-th tree with i-th cost\n    tree_opt<-rpart(Attrition_Flag ~ ., data = train, method = \"class\", parms=list(loss=c(0,costs_tree[i],1,0)))\n\n    # predict on test set\n    tree_opt_pred<- predict(tree_opt, newdata = test, type = \"class\") \n\n    # get confusion matrix\n    conf_matrix_tree_opt <- caret::confusionMatrix(tree_opt_pred, test$Attrition_Flag, positive = \"1\", mode = \"everything\")\n\n    # classification metrics\n    optimization_results_tree[i,1]<-costs_tree[i]\n    optimization_results_tree[i, 2:6] <- classification_metrics(conf_matrix_tree_opt)\n  \n}\n\n# Recall optimization results\noptimization_results_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_results_tree_plot_data <- optimization_results_tree %>%\n    pivot_longer(cols = c(-Cost), names_to = \"metric\", values_to = \"value\")\n\noptimization_results_tree_plot_data %>%\n    ggplot() +\n    geom_line(aes(x = Cost, y = value, color = metric), size = 1, alpha = 0.4) +\n    scale_x_continuous(breaks = scales::pretty_breaks(n = 20)) +\n    scale_y_continuous(breaks = scales::pretty_breaks(n = 20)) +\n    geom_text_repel(\n      data = optimization_results_tree_plot_data %>% filter(Cost %in% c(1.0, 1.3, 1.6, 2.0, 2.7, \"3.8\")) %>% mutate(value = round(value,3)), \n      aes(x = Cost, y = value, label = value, color = metric),\n      direction = \"y\",\n      show.legend = F\n    ) +  \n    theme_classic() +\n    scale_color_brewer(palette=\"Set1\") +\n    labs(\n      x = \"Cost\",\n      y = \"Value\",\n      color = \"Metric\",\n      title = \"Metrics trends by varying the missclassification cost - classification tree\"\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  optimization_results_tree_indexes_f1 <- c(7,11,18,29)\n  optimization_results_tree_indexes_f1_selected <- c(4)\n  \n  optimization_results_tree_results_pr <- optimization_results_tree\n  optimization_results_tree_results_pr <- optimization_results_tree_results_pr %>% \n    mutate(selected = if_else(row_number() %in% optimization_results_tree_indexes_f1_selected, 1, 0))\n  \n  optimization_results_tree_results_pr %>%\n    ggplot() +\n    geom_point(aes(x = Recall, y = Precision, color = F1), size = 4) +\n    geom_line(aes(x = Recall, y = Precision), size = 0.8, alpha = 0.15) +\n    geom_label_repel(\n      data = optimization_results_tree_results_pr %>% \n        filter(row_number() %in% c(optimization_results_tree_indexes_f1, optimization_results_tree_indexes_f1_selected)), \n      aes(\n        x = Recall, \n        y = Precision, \n        label = paste0(\"c = \", Cost), \n        fill = factor(selected)\n      ), \n      size = 4\n    ) +\n    theme_classic() +\n    scale_colour_gradientn(colours = c(\"#ff0000\",\"#ff9c00\",\"#fffca6\"),\n                           values = c(1.0,0.8,0))  +\n    scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +\n    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\n    scale_fill_manual(\n      values = c(\"grey\", \"white\"), \n      breaks = c(1, 0), \n      labels = c(1, 0)\n    ) +\n    labs(\n      x = \"Recall\",\n      y = \"Precision\",\n      title = \"Recall, Precision and F1 trends by varying the missclassification cost - classification tree\"\n    ) +\n    guides(fill=FALSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selected procedures\noptimization_results_tree[c(4,11,18,29),]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The procedures shown above are the most relevant procedures obtained by the cost-sensitivity optimization.\nThe procedure with a cost equal to 1.3 is the one with the highest F1 and with recall higher than precision. The procedures with costs equal to 2.0, 2.7, 3.8 offer higher recall but lower precision (and F1). "},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(123)\n\n# tree with cost 1.3\ntree_opt_1 <- rpart(Attrition_Flag ~ ., data = train, method = \"class\", parms=list(loss=c(0,1.3,1,0)))\ntree_opt_1_pred <- predict(tree_opt_1, newdata = test, type = \"class\") \nconf_matrix_tree_opt_1 <- caret::confusionMatrix(tree_opt_1_pred, test$Attrition_Flag, positive = \"1\", mode = \"everything\")\nmetrics_tree_opt_1 <- classification_metrics(conf_matrix_tree_opt_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick comparison with the **Pruned Tree** shows that a cost equal to 1.3 gives the **best precision-recall trade-off**, with comparable F1 and **higher recall**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pruned Tree\nconf_matrix_tree_0$table\nmetrics_tree_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tree with cost 1.3\nconf_matrix_tree_opt_1$table\nmetrics_tree_opt_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variables importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width = 18, repr.plot.height = 8)\n\ntree_opt_1_varimp <- data.frame(tree_opt_1$variable.importance)\ntree_opt_1_varimp$variable <- rownames(tree_opt_1_varimp)\nnames(tree_opt_1_varimp) <- c(\"importance\", \"variable\")\n\ntree_opt_1_varimp %>%\n    ggplot(aes(x = importance, y = reorder(variable, importance))) +\n    geom_bar(aes(fill = importance), stat = \"identity\", color = \"black\") +\n    geom_text(aes(label = round(importance, 3)), vjust = 0.20, hjust = -0.20) +\n    scale_fill_gradient(low = \"#c7e9c0\", high = \"#00441b\") +\n    theme_classic() +\n    labs(\n      title = \"Variables importance - Classification tree\",\n      x = \"Importance\",\n      y = \"Variable\",\n      fill = \"Importance\"\n    ) + guides(fill = FALSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **most important variables** obtained from the model **confirm the EDA**: *Total_Trans_Ct*, *Total_Trans_Amt* and *Total_Revolving_Bal* are the most important ones. Also, *Total_Ct_Chng_Q4_Q1* and *Total_Relationship_Count* appears in the top 6."},{"metadata":{},"cell_type":"markdown","source":"### Decision trees summary\nThe most relevant decision tree performances are set out below. \n\nThe tree with a cost equal to 1.3 gives the best performance (high F1 and recall higher than precision)."},{"metadata":{"trusted":true},"cell_type":"code","source":"comparison_tree_opt <- data.frame(matrix(nrow = 7, ncol = 6))\nnames(comparison_tree_opt) <- c(\"Cost\", \"Specificity\", \"Recall\", \"Precision\", \"F1\", \"Accuracy\")\n\ncomparison_tree_opt[1,1] <- \"Full Tree\"\ncomparison_tree_opt[1,2:6] <- metrics_tree_0\n\ncomparison_tree_opt[2,1] <- \"Pruned Tree\"\ncomparison_tree_opt[2,2:6] <- metrics_tree_1\n\ncomparison_tree_opt[3,1] <- \"Prior Tree\"\ncomparison_tree_opt[3,2:6] <- metrics_tree_2\n\ncomparison_tree_opt[4,] <- optimization_results_tree[4, ]\ncomparison_tree_opt[5,] <- optimization_results_tree[11, ]\ncomparison_tree_opt[6,] <- optimization_results_tree[18, ]\ncomparison_tree_opt[7,] <- optimization_results_tree[29, ]\n\ncomparison_tree_opt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting\nA gradient boosting model allows improving overall performances by iteratively building trees based on wrongly classified observations. This can be useful to improve minor class predictions in an unbalanced dataset as in the present case. \n\nThe model takes in input three parameters: **ðµ** (n.trees), **ðœ†** (shrinkage) and **ð‘‘** (interaction.depth). To optimize the recall, another parameter **ð‘** (cost) has been taken into account to assign a higher misclassification cost to False Negatives. By using a set of values for each parameter, a boosting procedure has been built for every possible combination of them. A fixed value has been assigned to **ðµ** for every procedure. This parameter has been optimized later by considering cross-validation error. In particular, stratified cross-validation has been used to improve the data representation for each fold."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"library(gbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gbm library requires a non-factor response variable\ntrain_boost <- train\ntest_boost <- test\n\n# convert factor to numeric (0 and 1)\ntrain_boost$Attrition_Flag <- as.numeric(train$Attrition_Flag) - 1\ntest_boost$Attrition_Flag <- as.numeric(test_boost$Attrition_Flag) - 1\n\ntable(train_boost$Attrition_Flag)\ntable(test_boost$Attrition_Flag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# metrics names\nmetrics_names_boost <- c(\n  \"cost\",\n  \"best.iteration\", \n  \"shrinkage\",  \n  \"interaction.depth\",\n  \"Specificity\",\n  \"Recall\",\n  \"Precision\",\n  \"F1\",\n  \"Accuracy\"\n)\n\n# gbm training helper method. It takes training set, test set and parameters in input and returns\n# a list containing the trained model and its performances.\ntrain_boosting_with_params <- function(train, test, shrinkage, interaction.depth, cost, n.trees) {\n  \n  boost_model_opt_proc <- list()\n  \n  set.seed(1)\n  \n  # gbm training\n  boost_model_opt_proc[[\"model\"]] <-gbm(\n      \n    Attrition_Flag~., \n    distribution = \"bernoulli\",\n    \n    data=train,                                # use the training set\n    train.fraction = 1,                        # use the whole training set\n    cv.folds = 5,                              # k-fold cross validation with 5 folds\n    class.stratify.cv = TRUE,                  # stratified cross validation\n    \n    # parameters ---------------------------------------------------------\n    shrinkage = shrinkage,                    \n    interaction.depth = interaction.depth,     \n    n.trees = n.trees,                        \n    \n    weights = ifelse(train$Attrition_Flag == 1, cost, 1) # missclassification cost\n    \n  )\n  \n  # get best iteration (n.trees) on cross-validation\n  boost_model_opt_proc[[\"best.iter\"]] <- gbm.perf(boost_model_opt_proc[[\"model\"]], method = \"cv\")[1]\n  \n  # predict on test set (probs)\n  boost_model_opt_proc[[\"pred.prob\"]] <- predict(\n      boost_model_opt_proc[[\"model\"]], \n      test, \n      n.trees = boost_model_opt_proc[[\"best.iter\"]], \n      type = \"response\"\n  )\n  \n  # set the prob cut-off at 0.5\n  boost_model_opt_proc[[\"cutoff\"]] <- 0.5\n  \n  # convert probs to classes\n  boost_model_opt_proc[[\"pred.class\"]] <- ifelse(boost_model_opt_proc[[\"pred.prob\"]] >  boost_model_opt_proc[[\"cutoff\"]],1,0)\n  boost_model_opt_proc[[\"pred.class\"]] <- as_factor(boost_model_opt_proc[[\"pred.class\"]])\n  \n  # get the confusion matrix\n  conf_mat_opt_proc <- caret::confusionMatrix(\n      boost_model_opt_proc[[\"pred.class\"]], \n      as_factor(test$Attrition_Flag), \n      positive = \"1\", \n      mode = \"everything\"\n  )\n  \n  # results\n  boost_model_opt_proc[[\"results\"]] <- data.frame(matrix(0, nrow=1, ncol=9)) \n  names(boost_model_opt_proc[[\"results\"]]) <- metrics_names_boost\n  \n  boost_model_opt_proc[[\"results\"]][1,1]<-cost \n  boost_model_opt_proc[[\"results\"]][1,2]<-boost_model_opt_proc[[\"best.iter\"]][1]\n  boost_model_opt_proc[[\"results\"]][1,3]<-shrinkage\n  boost_model_opt_proc[[\"results\"]][1,4]<-interaction.depth\n  boost_model_opt_proc[[\"results\"]][1,5:9] <- classification_metrics(conf_mat_opt_proc)\n  \n  return(boost_model_opt_proc)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimization procedures"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"options(repr.plot.width = 8, repr.plot.height = 6)\n\n# shrinkage tuning values\nshrinkage_parms <- c(0.05, 0.1, 0.2)\n\n# interaction.depth tuning values\ninteraction_depth_params <- c(1, 3, 4, 6)\n\n# cost tuning values\ncosts_boost_params <- c(1, 1.5, 2, 5, 7, 10)\n\n# boosting procedures list\nboost_optimization_models <- list()\n\nboost_opt_mod_index = 1\n\n# for each i-th shrinkage\nfor(i in c(1:length(shrinkage_parms))) {\n\n  # for each j-th interaction.depth\n  for(j in c(1:length(interaction_depth_params))) {\n\n    # for each k-th cost\n    for(k in c(1:length(costs_boost_params))) {\n\n      # boosting with i-th shrinkage, j-th interaction.depth and k-th cost.\n      # n.tree has a fixed value of 5000\n      boost_optimization_models[[boost_opt_mod_index]] <- train_boosting_with_params(\n          train_boost, \n          test_boost, \n          shrinkage_parms[i], \n          interaction_depth_params[j], \n          costs_boost_params[k], \n          5000\n      )\n      boost_opt_mod_index <- boost_opt_mod_index + 1\n    }\n\n  }\n\n} \n\n# matrix with boosting procedures\nboost_optimization_results <- data.frame(matrix(0, nrow=72, ncol=9)) \nnames(boost_optimization_results)<- metrics_names_boost\nboost_optimization_results$procedure <- c(1:72)\n\n# extract results for each procedure \nfor(i in c(1:72)) {\n  boost_optimization_results[i, 1:9] <- boost_optimization_models[[i]]$results\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# results\nboost_optimization_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_optimization_results %>% arrange(desc(F1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width = 18, repr.plot.height = 8)\n\nboost_optimization_results_indexes_f1 <- c(7,8,13,14,31,32,33,37,39,44,45,61,62,63,65,69,71)\nboost_optimization_results_indexes_f1_selected <- c(17,23,46,55,56)\n\nboost_optimization_results_pr <- boost_optimization_results\nboost_optimization_results_pr <- boost_optimization_results_pr %>% \nmutate(selected = if_else(row_number() %in% boost_optimization_results_indexes_f1_selected, 1, 0))\n\nboost_optimization_results_pr %>%\n    ggplot() +\n    geom_point(aes(x = Recall, y = Precision, color = F1), size = 4) +\n    geom_line(aes(x = Recall, y = Precision), size = 0.8, alpha = 0.15) +\n    geom_label_repel(\n      data = boost_optimization_results_pr %>% \n        filter(\n            row_number() %in% c(\n                boost_optimization_results_indexes_f1, \n                boost_optimization_results_indexes_f1_selected\n            )\n        ), \n      aes(\n        x = Recall, \n        y = Precision, \n        label = paste0(shrinkage, \", \", interaction.depth, \", \", cost), \n        fill = factor(selected)\n      ), \n      size = 2.7\n    ) +\n    theme_classic() +\n    scale_colour_gradientn(colours = c(\"#ff0000\",\"#ff9c00\",\"#fffca6\"), values = c(1.0,0.87,0))  +\n    scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +\n    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\n    scale_fill_manual(\n      values = c(\"grey\", \"white\"), \n      breaks = c(1, 0), \n      labels = c(1, 0)\n    ) +\n    labs(\n      x = \"Recall\",\n      y = \"Precision\",\n      title = \"Recall, Precision and F1 trends by varying the missclassification cost - classification tree\"\n    ) +\n    guides(fill=FALSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_optimization_results_pr %>%\n    ggplot() +\n    geom_point(aes(x = procedure, y = F1, color = factor(selected))) +\n    geom_line(aes(x = procedure, y = F1), alpha = 0.25) +\n    geom_label_repel(\n        data = boost_optimization_results_pr %>% \n        filter(\n            row_number() %in% c(\n                boost_optimization_results_indexes_f1, \n                boost_optimization_results_indexes_f1_selected\n            )\n        ), \n        aes(\n            x = procedure, \n            y = F1, \n            label = paste0(shrinkage, \", \", interaction.depth, \", \", cost),\n            fill = factor(selected)\n        ), \n        size = 2.5\n    ) +\n    theme_classic() +\n    scale_x_continuous(breaks = scales::pretty_breaks(n = 36)) +\n    scale_fill_manual(\n        values = c(\"grey\", \"white\"), \n        breaks = c(1, 0), \n        labels = c(1, 0)\n    ) +\n    scale_color_manual(\n        values = c(\"black\", \"grey\"), \n        breaks = c(1, 0), \n        labels = c(1, 0)\n    ) +\n    guides(fill=FALSE, color = FALSE) +\n    labs(\n        x = \"Procedure\",\n        title = \"F1 trend by varying procedures\"\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most relevant boosting procedures\nboost_optimization_results[c(6,17,23,46,55,56),] %>% arrange(desc(F1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results above show the most relevant boosting procedures based on F1, Recall, Precision and model complexity (n.trees).\nProcedures 56 and 55 are those with the **highest F1**. Procedure 55 is akin to a boosting without any cost-sensitivity (cost = 1.0). Procedure 56 has the highest F1 and the lowest number of trees (404) and for this reason it's been considered the **best F1 choice**.\n\nThe procedures 23, 46, and 17 have F1 comparable to procedure 56, but with recall higher than precision. Among these three procedures, procedure 23 has the highest F1. Procedure 6 is the one with the highest recall among the 72 procedures, but with a more drastic decrease in precision and F1, which suggests a not \"fair\" trade-off. \n\nBy comparing the procedure 56 and 23, this last one has **F1 comparable** to procedure 56 but with **recall higher than precision**. For that reason, procedure 23 has been considered the **best trade-off**."},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(1)\n\n# boosting with procedure 56 parameters\nbest_boost_model_proc_56 <- gbm(\n  Attrition_Flag~., \n  distribution = \"bernoulli\",\n  data=train_boost, \n  train.fraction = 1, \n  cv.folds = 5,\n  class.stratify.cv = TRUE,\n  \n  shrinkage = 0.2,\n  interaction.depth = 3, \n  n.trees = 404,\n  weights = ifelse(train$Attrition_Flag == 1, 1.5, 1)\n)\n\n# predict on test set with the best iteration (n.trees)\nbest_boost_model_proc_56_pred <- predict(best_boost_model_proc_56,test_boost,n.trees = 404, type = \"response\")\nbest_boost_model_proc_56_pred_factor <- ifelse(best_boost_model_proc_56_pred > 0.5,1,0)\nbest_boost_model_proc_56_pred_factor <- as_factor(best_boost_model_proc_56_pred_factor)\ntest_boost_factor <- as_factor(test_boost$Attrition_Flag)\n\n# get confusion matrix\nconf_matrix_best_boost_model_proc_56 <- caret::confusionMatrix(best_boost_model_proc_56_pred_factor, test_boost_factor, positive = \"1\", mode = \"everything\")\nmetrics_best_boost_model_proc_56 <- classification_metrics(conf_matrix_best_boost_model_proc_56)\n\ngbm.perf(boost_optimization_models[[56]]$model, method = \"cv\")\ntitle(main = \"Boosting 56\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(1)\n\n# boosting with procedure 23 parameters\nbest_boost_model_proc_23 <- gbm(\n  Attrition_Flag~., \n  distribution = \"bernoulli\",\n  data=train_boost, \n  train.fraction = 1, \n  cv.folds = 5,\n  class.stratify.cv = TRUE,\n  \n  shrinkage = 0.05,\n  interaction.depth = 6, \n  n.trees = 1130,\n  weights = ifelse(train$Attrition_Flag == 1, 7.0, 1)\n)\n\n# predict on test set with the best iteration (n.trees)\nbest_boost_model_proc_23_pred <- predict(best_boost_model_proc_23,test_boost,n.trees=1130, type = \"response\")\n\nbest_boost_model_proc_23_pred_factor <- ifelse(best_boost_model_proc_23_pred > 0.5,1,0)\nbest_boost_model_proc_23_pred_factor <- as_factor(best_boost_model_proc_23_pred_factor)\ntest_boost_factor <- as_factor(test_boost$Attrition_Flag)\n\n# get confusion matrix\nconf_matrix_best_boost_model_proc_23 <- caret::confusionMatrix(best_boost_model_proc_23_pred_factor, test_boost_factor, positive = \"1\", mode = \"everything\")\nmetrics_best_boost_model_proc_23 <- classification_metrics(conf_matrix_best_boost_model_proc_23)\n\ngbm.perf(boost_optimization_models[[23]]$model, method = \"cv\")\ntitle(main = \"Boosting 23\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boosting procedure 56\nconf_matrix_best_boost_model_proc_56$table\nmetrics_best_boost_model_proc_56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boosting procedure 23\nconf_matrix_best_boost_model_proc_23$table\nmetrics_best_boost_model_proc_23","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variables importance"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Variables importance for boosting procedure 23\nbest_boost_model_proc_23_varimp <- data.frame(summary(best_boost_model_proc_23, n.trees = 1130))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_boost_model_proc_23_varimp %>%\n    ggplot(aes(x = rel.inf, y = reorder(var, rel.inf))) +\n    geom_bar(aes(fill = rel.inf), stat = \"identity\", color = \"black\") +\n    geom_text(aes(label = round(rel.inf, 3)), vjust = 0.20, hjust = -0.20) +\n    scale_fill_gradient(low = \"#c7e9c0\", high = \"#00441b\") +\n    theme_classic() +\n    labs(\n      title = \"Variables importance - Boosting procedure 46\",\n      x = \"Importance\",\n      y = \"Variabile\",\n      fill = \"Importance\"\n    ) + guides(fill = FALSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly to the decision tree, variables importance for the selected boosting procedure confirms the EDA, with higher importance on *Total_Trans_Ct*, *Months_Inactive_12_mon*, and *Contacts_Count_12_mon*. \n\nThe plot below shows the marginal effect of the most important variables on the response variable *Attrition_Flag*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"effect_1 <- plot(best_boost_model_proc_23,i=\"Total_Trans_Ct\", type = \"response\", main = \"Marginal effect of Total_Trans_Ct on Attrition_Flag\")\neffect_2 <- plot(best_boost_model_proc_23,i=\"Total_Trans_Amt\", type = \"response\", main = \"Marginal effect of Total_Trans_Amt on Attrition_Flag\")\neffect_3 <- plot(best_boost_model_proc_23,i=\"Total_Revolving_Bal\", type = \"response\", main = \"Marginal effect of Total_Revolving_Bal on Attrition_Flag\")\neffect_4 <- plot(best_boost_model_proc_23,i=\"Total_Ct_Chng_Q4_Q1\", type = \"response\", main = \"Marginal effect of Total_Ct_Chng_Q4_Q1 on Attrition_Flag\")\neffect_5 <- plot(best_boost_model_proc_23,i=\"Total_Relationship_Count\", type = \"response\", main = \"Marginal effect of Total_Relationship_Count on Attrition_Flag\")\neffect_6 <- plot(best_boost_model_proc_23,i=\"Total_Amt_Chng_Q4_Q1\", type = \"response\", main = \"Marginal effect of Total_Amt_Chng_Q4_Q1 on Attrition_Flag\")\ngrid.arrange(effect_1, effect_2, effect_3, effect_4, effect_5, effect_6, nrow = 2, ncol = 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting summary\nThe most relevant boosting procedures performances are set out below. \n\nThe boosting procedure 23 gives the best performance (high F1 and recall higher than precision)."},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_optimization_results[c(6,17,23,46,55,56),] %>% arrange(desc(F1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparison and Conclusion\nBy comparing the selected decision tree and the selected boosting procedure, this last one performs better in every metric:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision tree with cost 1.3\nconf_matrix_tree_opt_1$table\nmetrics_tree_opt_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boosting procedure 23 with shrinkage = 0.05, interaction.depth = 6, n.trees = 1130 and cost = 7.0\nconf_matrix_best_boost_model_proc_23$table\nmetrics_best_boost_model_proc_23","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In spite of better performance, model interpretability must be taken into account: a gradient boosting model is **less interpretable** than a single decision tree due to its ensemble nature. The selected decision tree has already a good performance, so it may not worth losing interpretability in favor of a better performance. Moreover, having more information about the economic impact of offers proposed to customers could lead to a better precision-recall trade-off."}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}